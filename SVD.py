from scipy.sparse.linalg import norm as sparse_norm
from scipy.sparse import vstack, lil_array, csr_array
from collections import defaultdict
import time
import random
import math
import numpy as np

import numba as nb
from numba import jit, njit, float64, uint32
from numba.typed import List, Dict

class Metrics:
    def rmse(self, predictions):
        return math.sqrt(sum((prediction - true_rating)**2 for _, _, prediction, 
                             true_rating in predictions)/len(predictions))

class SVDBase():
    """Base class for funk svd"""
    def __init__(self, num_users, num_items, num_ratings, k=10, learning_rate=0.01,
                  C=0.02):
        """Initialise new SVDBase.
        
        Parameters:
            num_user (int) - Number of users
            num_items (int) - Number of items
            num_ratings (int) - The number of different possible ratings
            k (int) - The number of latent factors
            learning_rate (float) - The learning rate
            C (float) - Regularization parameter
            partial"""
        self._num_users = num_users
        self._num_items = num_items
        self._num_ratings = num_ratings
        
        self._k = k
        self._learning_rate = learning_rate
        self._C = C
        self._lrate_C = self._learning_rate*self._C
        
        self._user_features = np.array(np.random.normal(size=(self._num_users, self._k), 
                                               scale=0.01), dtype=np.float64)
        self._item_features = np.array(np.random.normal(size=(self._num_items, self._k), 
                                               scale=0.01), dtype=np.float64)
        
        self._M = None
        self._num_samples = None
        self._train_errors = None
        self._val_errors = None
        self._epoch = -1

    def fit(self, M, epochs, validation_set=None, tol=1e-15, early_stop=True):
        """Fit the model with the given utility matrix M (csr array).
        
        Parameters:
            M (scipy csr_array) - UxI array where U is the number of users
                and I is the number of items
            epochs (int) - The number of epochs
            validation_set (list) - List of validation pairs
            tol (float) - Early stopping tolerance
            early_stop (bool) - True to enable automatic early stopping"""
        self._M = M 
        
        self._validation_set = validation_set
        self._tol = tol
        self._train_errors = []
        if validation_set:
            self._val_errors = []
        
        # Retrieve sample locations
        self._users, self._items = self._M.nonzero()
        self._num_samples = len(self._users)
        self._mask = (self._M != 0)

        self._cache_users_rated()

        self._run_epochs(self._users, self._items, epochs, early_stop=early_stop)

    def continue_fit(self, epochs, early_stop=True):
        """Continue training for extra epochs"""          
        self._run_epochs(self._users, self._items, epochs, early_stop=early_stop)

    def partial_fit(self, new_sample, epochs, batch_size=0, compute_err=False):
        """"Faciliates online training. Add new user vector new_sample into the 
        model and fit with warm start.
        
        Parameters:
            new_sample (csr_array) - 1xI arrary where I is the number of items
            epochs (int) - The number of epochs
            batch_size (int) - The number of users to mix-in for training
            compute_err (bool) - True to enable training error computation"""
        self._num_users += 1
        self._M = csr_array(vstack([lil_array(self._M), new_sample]))
        total_users, total_items = self._M.nonzero()
        self._users, self._items = total_users, total_items
        self._num_samples = len(total_users)
        
        self._mask = (self._M != 0)
        
        self._user_features = np.concatenate(
            [self._user_features, np.random.normal(size=(1, self._k), scale=0.01)], axis=0)
                                                                               
        indices_of_new = [new_i for new_i in range(self._num_users - 1, len(total_users))]

        self._cache_users_rated()

                                              
        for epoch in range(epochs):
            start_time = time.time()
            # Choose a smaller subset of total samples already fitted
            fitted_subset = random.sample(range(self._num_samples), k=batch_size)    
            
            # Ensure that new indices are always used
            possible_indices = fitted_subset + indices_of_new
            
            # Perform update for each sample
            for i in random.sample(possible_indices , k=len(possible_indices)):
                self._user_features, self._item_features = (
                    update_fast(i, total_users[i], total_items[i], self._M.data, 
                                self._user_features,
                                self._item_features,
                                self._learning_rate,
                                self._lrate_C)
                ) 
            
            print("Epoch", epoch, end="/")
            if compute_err:
                self._compute_error()
            print("Time:", round(time.time() - start_time, 2), "seconds")

    def pop_user(self):
        """Remove the last added user from the model."""
        self._num_users -= 1
        self._M = self._M[:-1, :]        
        self._user_features = self._user_features[:-1, :]

    def top_n(self, user, n=10):
        """Return the top n recommendations for given user.
        
        Parameters:
            user (int) - The index of the user
            n (int) - The number of recommendations to give
            
        Preconditions:
            n > 0"""        
        top = []
        for item in range(self._num_items):
            # Do not add items for which rating already exists
            if user in self._users_rated and item in self._users_rated[user]:
                continue
                
            predicted_rating = self.predict(user, item)
            
            top.append((predicted_rating, item))
            top.sort(key=lambda x: x[0], reverse=True)
            top = top[:min(n, len(top))]
        
        return top
    
    def predict_pairs(self, pairs):
        """Returns a list of predictions of the form (user, item, prediction) 
        for each (user, item) pair in pairs.
        
        Parameters:
            pairs (list) - List of (user, item) tuples.
            
        Returns:
            List of (user, item, prediction) tuples."""
        predictions = []
        for user, item in pairs:
            prediction = self.predict(user, item)
            predictions.append((user, item, prediction))
        
        return predictions

    def get_train_errors(self):
        """Return the training errors stored while training. Returns none if 
        model has not been fit."""
        return self._train_errors
    
    def get_val_errors(self):
        """Return the validation errors stored while training. Returns none if 
        model has not been fit."""
        return self._val_errors

    def _cache_users_rated(self):
        self._users_rated = {}
        for sample_num in range(self._num_samples):
            user = self._users[sample_num]
            item = self._items[sample_num]
            if user not in self._users_rated:
                self._users_rated[user] = []
            self._users_rated[user].append(item)

    def _run_epochs(self, users, items, epochs, early_stop=False):
        for epoch in range(epochs):
            start_time = time.time()
            
            # For all samples in random order update each parameter
            for i in random.sample(range(self._num_samples), k=self._num_samples):
                self._update_features(i, users, items)     
            
            # Display training information
            print("Epoch", epoch, end="/")
            self._compute_error()
            
            if self._validation_set:
                self._compute_val_error()
                
            print("Time:", round(time.time() - start_time, 2), "seconds")
            
            # Convergence criterion
            if (self._validation_set 
                and early_stop 
                and len(self._val_errors) > 1 
                and self._val_errors[-2] - self._val_errors[-1] < self._tol
            ):
                    print("Small change in validation error. Terminating training.")
                    return
            
            
class SVDPredictor(SVDBase):
    """SVD for collaborative filtering, uses explicit ratings."""
    def __init__(self, num_users, num_items, num_ratings, k=10, learning_rate=0.01,
                  C=0.02):
        super().__init__(num_users, num_items, num_ratings, k=k, 
                          learning_rate=learning_rate, C=C)
        
        self._user_biases = np.zeros([self._num_users, 1])
        self._item_biases = np.zeros([self._num_items, 1])
        self._item_implicit = np.random.normal(size=(self._num_items, self._k), 
                                                   scale=0.01)
        self._user_implicit = np.random.normal(size=(self._num_users, self._k), 
                                                   scale=0.01)
    
    def predict(self, user, item):
        """Predict users rating of item. User and item are indices corresponding
        to user-item matrix."""
        return (self._mu 
                + self._user_biases[user, 0] 
                + self._item_biases[item, 0] 
                + (self._user_features[user, :] 
                   + self._user_implicit_features(user))
                @ np.transpose(self._item_features[item, :])
                )
    
    def _update_features(self, i, users, items, do_items=True):
        user = users[i]
        item = items[i]
        self._user_implicit[user, :] = self._user_implicit_features(user)                  
        diff = self._M[user, item] - self.predict(user, item)

        # Compute user bias update
        self._user_biases[user, 0] += self._learning_rate*(
            diff - self._C*self._user_biases[user, 0])
        
        # Compute user features update
        new_user_features = (self._user_features[user, :] + self._learning_rate*(
            diff*self._item_features[item, :] 
            - self._C*self._item_features[item, :]))
        
        if do_items:
            # Compute item features update
            new_item_features = self._item_features[item, :] + self._learning_rate*(
                diff*(self._user_features[user, :] 
                      + self._user_implicit[user, :])
                - self._C*self._user_features[user, :])
            
            # Compute item bias update
            self._item_biases[item, 0] += self._learning_rate*(
                diff - self._C*self._item_biases[item, 0])
            
            # Compute implicit item feature update
            self._item_implicit[item, :] += self._learning_rate*(
                diff*self._item_features[item, :]/np.sqrt(len(self._users_rated[user]))
                - self._C*self._user_implicit[user, :]
            )            
            
        self._user_features[user, :] = new_user_features
        self._item_features[item, :] = new_item_features

    def _user_implicit_features(self, user):
        user_implicit = (np.sum(
            np.concatenate(
                [self._item_implicit[item_star, :] for item_star in self._users_rated[user]], 
                axis=0), axis=0) 
        )

        user_implicit /= np.sqrt(len(self._users_rated[user]))

        return user_implicit
              
    def _compute_error(self):
        # Update all user implicits
        for user in range(self._num_users):
            self._user_implicit[user, :] = self._user_implicit_features(user)

        estimate_M = (
            self._mask.multiply(self._mu)
            + self._mask.multiply(np.repeat(self._user_biases, self._M.shape[1], 
                                            axis=1))
            + self._mask.multiply(np.repeat(np.transpose(self._item_biases), 
                                            self._M.shape[0], axis=0))
            + self._mask.multiply((self._user_features + self._user_implicit) 
                                  @ np.transpose(self._item_features))
        )
        big_diff = self._M - estimate_M
        
        error = sparse_norm(big_diff) / np.sqrt(self._num_samples)
        self._train_errors.append(error)
        print("Training error:", error, end="/")

    def _compute_val_error(self):
        # Predict rating for all pairs in validation
        predictions = self.predict_pairs([(user, item) 
                                    for user, item, _ in self._validation_set])
        
        # Add true ratings into tuples
        predictions = [prediction + (self._validation_set[i][2],) 
                        for i, prediction in enumerate(predictions)]
        
        metrics = Metrics()
        val_error = metrics.rmse(predictions)
        self._val_errors.append(val_error)
        print("Validation error:", val_error, end="/")

class LogisticSVD(SVDBase):
    """FunkSVD using binary classification for like and dislike."""
    def predict(self, user, item):
        """Predict users rating of item. User and item are indices corresponding
        to user-item matrix."""
        return self._sigmoid(
                self._user_features[user, :]   
                @ np.transpose(self._item_features[item, :])
                )

    def _update_features(self, i, users, items, do_items=True):
        user = users[i]
        item = items[i]

        # Pre-cache computations
        true = self._M[user, item] - 1
        pred = self.predict(user, item)
        a = np.exp(-self._user_features[user, :] 
                   @ np.transpose(self._item_features[item, :]))
        ab = a*pred
        coeff = self._learning_rate*( 
            ( -(1 - true)*ab*pred )/(1 - pred) + true*ab 
            )
        
        # Compute user features update
        new_user_features = (
            self._user_features[user, :] + self._item_features[item, :]*coeff
            -self._lrate_C*self._user_features[user, :]
        )
        
        if do_items:
            # Compute item features update
            new_item_features = (
                self._item_features[item, :] + self._user_features[user, :]*coeff
                -self._lrate_C*self._item_features[item, :]
            )
            
        self._user_features[user, :] = new_user_features
        if do_items:
            self._item_features[item, :] = new_item_features

    def _sigmoid(self, x):
        return 1/(1 + np.exp(-x))
    
    def _compute_error(self):
        loss = 0
        for user in self._users_rated:
            items = self._users_rated[user]
            for item in items:
                true = self._M[user, item] - 1
                pred = self.predict(user, item)

                loss += true*np.log(pred) + (1 - true)*np.log(1 - pred)

        loss *= -(1/self._num_samples)
        self._train_errors.append(loss)
        print("Training error:", loss, end="/")
    
    def _compute_val_error(self):
        # Predict rating for all pairs in validation
        predictions = self.predict_pairs([(user, item) 
                                    for user, item, _ in self._validation_set])
        
        # Add true ratings into tuples
        predictions = [prediction + (self._validation_set[i][2],) 
                        for i, prediction in enumerate(predictions)]
        
        val_error = 0
        for user, item, pred, true in predictions:
            val_error += true*np.log(pred) + (1 - true)*np.log(1 - pred)

        val_error *= -(1/len(predictions))
        self._val_errors.append(val_error)
        print("Validation error:", val_error, end="/")

class FastLogisticSVD(LogisticSVD):
    def fit(self, M, epochs, validation_set=None, tol=1e-15, early_stop=True):
        self._M = M 
        self._M = self._M.tocsr()
        
        self._validation_set = validation_set
        self._tol = tol
        self._train_errors = np.zeros([epochs])
        if validation_set:
            self._val_errors = np.zeros([epochs])
        
        # Retrieve sample locations
        self._users, self._items = self._M.nonzero()
        self._num_samples = len(self._users)
        self._mask = (self._M != 0)

        self._cache_users_rated()

        self._run_epochs(self._users, self._items, epochs, early_stop=early_stop)

    def continue_fit(self, epochs, early_stop=True):
        """Continue training for extra epochs"""      
        new_train_errors = np.zeros([self._train_errors.shape[0] + epochs])
        new_val_errors = np.zeros([self._train_errors.shape[0] + epochs])   

        new_train_errors[:self._train_errors.shape[0]] = self._train_errors
        new_val_errors[:self._val_errors.shape[0]] = self._val_errors

        self._train_errors = new_train_errors
        self._val_errors = new_val_errors
        self._run_epochs(self._users, self._items, epochs, early_stop=early_stop)

    def predict(self, user, item):
        return predict_fast(user, item, self._user_features, 
                                  self._item_features)
    
    def _cache_users_rated(self):
        self._users_rated = {}
        for sample_num in range(self._num_samples):
            user = self._users[sample_num]
            item = self._items[sample_num]
            if user not in self._users_rated:
                self._users_rated[user] = []
            self._users_rated[user].append(item)
    
    def _run_epochs(self, users, items, epochs, early_stop=False):
        self._M = csr_array(self._M)
        for epoch in range(epochs):
            self._epoch += 1
            start_time = time.time()
            
            # For all samples in random order update each parameter
            for i in random.sample(range(self._num_samples), k=self._num_samples):
                self._user_features, self._item_features = (
                    update_fast(i, users[i], items[i], self._M.data, 
                                self._user_features,
                                self._item_features,
                                self._learning_rate,
                                self._lrate_C)
                ) 
            
            # Display training information
            print("Epoch", epoch, end="/")
            loss = self._compute_error()
            print("Training error:", loss, end="/")

            if self._validation_set:
                val_loss = self._compute_val_error()
                print("Validation error:", val_loss, end="/")
                
            print("Time:", round(time.time() - start_time, 2), "seconds")
            
            # Convergence criterion
            if (self._validation_set 
                and early_stop 
                and epoch > 1 
                and self._val_errors[-2] - self._val_errors[-1] < self._tol
            ):
                    print("Small change in validation error. Terminating training.")
                    return

    def _compute_error(self):
        self._M = self._M.tocsr()
        return compute_error_fast(self._M.data, self._M.indices, self._M.indptr, 
                           self._num_samples, self._train_errors, self._epoch,
                           self._user_features, self._item_features)
        
    def _compute_val_error(self):
        return compute_val_error_fast(self._val_errors, List(self._validation_set), self._epoch,
                               self._user_features, self._item_features)
        
@jit(nopython=True)
def update_fast(i, user, item, values, user_features, item_features, learning_rate, lrate_C, do_items=True):
    # Pre-cache computations
    true = values[i] - 1
    pred = predict_fast(user, item, user_features, item_features)
    a = np.exp(-np.dot(user_features[user, :], item_features[item, :]))
    ab = a*pred
    coeff = learning_rate*( 
        ( -(1 - true)*ab*pred )/(1 - pred) + true*ab 
        )
    
    # Compute user features update
    new_user_features = (
        user_features[user, :] + item_features[item, :]*coeff
        -lrate_C*user_features[user, :]
    )
    
    if do_items:
        # Compute item features update
        new_item_features = (
            item_features[item, :] + user_features[user, :]*coeff
            -lrate_C*item_features[item, :]
        )
        item_features[item, :] = new_item_features
    
    user_features[user, :] = new_user_features

    return user_features, item_features

@jit(nopython=True)
def compute_error_fast(values, indices, indptr, num_samples, train_errors, 
                       epoch, user_features, item_features):
    loss = 0

    num_vals = 0
    next_row_index = 0
    for i in range(len(values)):
        value = values[i]
        column = indices[i]

        num_vals += 1
        if indptr[next_row_index] < num_vals:
            next_row_index += 1

        true = value - 1
        pred = predict_fast(next_row_index - 1, column, user_features, item_features)

        loss += true*np.log(pred) + (1 - true)*np.log(1 - pred)

    loss *= -(1/num_samples)
    train_errors[epoch] = loss

    return loss

@jit(nopython=True)
def compute_val_error_fast(val_errors, validation_set,
                       epoch, user_features, item_features):
   # Predict rating for all pairs in validation
    predictions = predict_pairs_fast([(user, item) 
                                for user, item, _ in validation_set], 
                                user_features, item_features)
    
    # Add true ratings into tuples
    predictions = [prediction + (validation_set[i][2],) 
                    for i, prediction in enumerate(predictions)]
    
    val_error = 0
    for user, item, pred, true in predictions:
        
        val_error += true*np.log(pred) + (1 - true)*np.log(1 - pred)

    val_error *= -(1/len(predictions))
    val_errors[epoch] = val_error

    return val_error

@jit(nopython=True)
def sigmoid_fast(x):
    if np.isnan(x):
        print("x is nan!!!!!!!!")
    return 1/(1 + np.exp(-x))

@jit(nopython=True)
def predict_fast(user, item, user_features, item_features):
    sig = sigmoid_fast(
            np.dot(user_features[user, :], item_features[item, :])
            )
    sig = np.minimum(sig, 0.9999)
    sig = np.maximum(sig, 0.00001)
    if np.isnan(sig):
        sig = 0.9999
    return sig

@jit(nopython=True)
def predict_pairs_fast(pairs, user_features, item_features):
        """Returns a list of predictions of the form (user, item, prediction) 
        for each (user, item) pair in pairs.
        
        Parameters:
            pairs (list) - List of (user, item) tuples.
            
        Returns:
            List of (user, item, prediction) tuples."""
        predictions = []
        for user, item in pairs:
            prediction = predict_fast(user, item, user_features, item_features)
            predictions.append((user, item, prediction))
        
        return predictions



